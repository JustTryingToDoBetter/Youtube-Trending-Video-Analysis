{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51534c19",
   "metadata": {},
   "source": [
    "##Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba0ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.0.43)\n",
      "Requirement already satisfied: psycopg2-binary in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.12/site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from SQLAlchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from SQLAlchemy) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install SQLAlchemy psycopg2-binary pandas python-dateutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e32a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL 16.10 (Ubuntu 16.10-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "PG_USER = \"postgres\"\n",
    "PG_PWD  = \"postgres\"\n",
    "PG_HOST = \"127.0.0.1\"\n",
    "PG_PORT = 5432\n",
    "DB_NAME = \"4260354_gb_youtube_trends\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{PG_USER}:{PG_PWD}@{PG_HOST}:{PG_PORT}/{DB_NAME}\")\n",
    "with engine.connect() as conn:\n",
    "    print(conn.execute(text(\"select version()\")).scalar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4866a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS gb_videos CASCADE;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b8de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exists? True GBvideos.csv\n",
      "Raw rows: 38916 | columns: ['video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description']\n",
      "Parsed trending_date non-null: 38916 of 38916\n",
      "After dropna on keys: 38916 (dropped 0)\n",
      "After de-dup keys: 38742\n",
      "Inserted rows: 38742\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "from sqlalchemy import text, Text, Integer, BigInteger, Boolean, Date, DateTime\n",
    "\n",
    "csv_path = \"GBvideos.csv\"  # <-- set correctly\n",
    "print(\"CSV exists?\", os.path.exists(csv_path), csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(\"Raw rows:\", len(df), \"| columns:\", list(df.columns))\n",
    "\n",
    "# Robust date parsing (handles multiple common formats)\n",
    "def parse_trending_series(s):\n",
    "    c = pd.to_datetime(s, format=\"%y.%d.%m\", errors=\"coerce\")\n",
    "    if c.notna().sum() == 0:\n",
    "        c = pd.to_datetime(s, format=\"%y.%m.%d\", errors=\"coerce\")\n",
    "    if c.notna().sum() == 0:\n",
    "        c = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return c\n",
    "\n",
    "df[\"trending_date\"] = parse_trending_series(df[\"trending_date\"]).dt.date\n",
    "if \"publish_time\" in df.columns:\n",
    "    df[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "for col in [\"comments_disabled\",\"ratings_disabled\",\"video_error_or_removed\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "for col in [\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_id\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Parsed trending_date non-null:\", df[\"trending_date\"].notna().sum(), \"of\", len(df))\n",
    "\n",
    "# Ensure table exists (no unique index yet to avoid conflicts on first load)\n",
    "ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gb_videos (\n",
    "  id BIGSERIAL PRIMARY KEY,\n",
    "  video_id TEXT NOT NULL,\n",
    "  trending_date DATE NOT NULL,\n",
    "  title TEXT,\n",
    "  channel_title TEXT,\n",
    "  category_id INT,\n",
    "  publish_time TIMESTAMPTZ,\n",
    "  tags TEXT,\n",
    "  views BIGINT,\n",
    "  likes BIGINT,\n",
    "  dislikes BIGINT,\n",
    "  comment_count BIGINT,\n",
    "  thumbnail_link TEXT,\n",
    "  comments_disabled BOOLEAN,\n",
    "  ratings_disabled BOOLEAN,\n",
    "  video_error_or_removed BOOLEAN,\n",
    "  description TEXT\n",
    ");\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(ddl))\n",
    "\n",
    "# Filter to known columns and drop empties on key fields\n",
    "dtype_map = {\n",
    "    \"video_id\": Text(),\n",
    "    \"trending_date\": Date(),\n",
    "    \"title\": Text(),\n",
    "    \"channel_title\": Text(),\n",
    "    \"category_id\": Integer(),\n",
    "    \"publish_time\": DateTime(timezone=True),\n",
    "    \"tags\": Text(),\n",
    "    \"views\": BigInteger(),\n",
    "    \"likes\": BigInteger(),\n",
    "    \"dislikes\": BigInteger(),\n",
    "    \"comment_count\": BigInteger(),\n",
    "    \"thumbnail_link\": Text(),\n",
    "    \"comments_disabled\": Boolean(),\n",
    "    \"ratings_disabled\": Boolean(),\n",
    "    \"video_error_or_removed\": Boolean(),\n",
    "    \"description\": Text(),\n",
    "}\n",
    "\n",
    "cols = [c for c in dtype_map if c in df.columns]\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"video_id\",\"trending_date\"])\n",
    "print(\"After dropna on keys:\", len(df), f\"(dropped {before-len(df)})\")\n",
    "df = df.drop_duplicates(subset=[\"video_id\",\"trending_date\"])\n",
    "print(\"After de-dup keys:\", len(df))\n",
    "\n",
    "df[cols].to_sql(\n",
    "    \"gb_videos\",\n",
    "    engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    dtype=dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=10000,\n",
    ")\n",
    "print(\"Inserted rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796e97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count now: 38742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>views</th>\n",
       "      <th>publish_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r63VBOagGAo</td>\n",
       "      <td>Shawn Mendes x Portugal (FPF Official World Cu...</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>653114</td>\n",
       "      <td>2018-06-13 13:11:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YQJmvXamKYg</td>\n",
       "      <td>Conway: People are bending to the will of Pres...</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>99048</td>\n",
       "      <td>2018-06-13 12:56:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-QPdRfqTnt4</td>\n",
       "      <td>Dumbo Official Teaser Trailer</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>4427381</td>\n",
       "      <td>2018-06-13 07:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6h8QgZF5Qu4</td>\n",
       "      <td>Drop the Mic w/ Ashton Kutcher &amp; Sean Diddy Combs</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>864189</td>\n",
       "      <td>2018-06-13 05:27:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arY6lepNdzU</td>\n",
       "      <td>E3 2018 Exclusive Gameplay Demos, Interviews a...</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>349122</td>\n",
       "      <td>2018-06-13 04:09:23+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  r63VBOagGAo  Shawn Mendes x Portugal (FPF Official World Cu...   \n",
       "1  YQJmvXamKYg  Conway: People are bending to the will of Pres...   \n",
       "2  -QPdRfqTnt4                      Dumbo Official Teaser Trailer   \n",
       "3  6h8QgZF5Qu4  Drop the Mic w/ Ashton Kutcher & Sean Diddy Combs   \n",
       "4  arY6lepNdzU  E3 2018 Exclusive Gameplay Demos, Interviews a...   \n",
       "\n",
       "  trending_date    views              publish_time  \n",
       "0    2018-06-14   653114 2018-06-13 13:11:56+00:00  \n",
       "1    2018-06-14    99048 2018-06-13 12:56:49+00:00  \n",
       "2    2018-06-14  4427381 2018-06-13 07:00:00+00:00  \n",
       "3    2018-06-14   864189 2018-06-13 05:27:27+00:00  \n",
       "4    2018-06-13   349122 2018-06-13 04:09:23+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.begin() as conn:\n",
    "    n = conn.execute(text(\"SELECT COUNT(*) FROM gb_videos\")).scalar()\n",
    "    print(\"Row count now:\", n)\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE UNIQUE INDEX IF NOT EXISTS ux_gb_videos_vid_trend\n",
    "          ON gb_videos (video_id, trending_date);\n",
    "        CREATE INDEX IF NOT EXISTS ix_gb_videos_category_id ON gb_videos (category_id);\n",
    "        CREATE INDEX IF NOT EXISTS ix_gb_videos_publish_time ON gb_videos (publish_time);\n",
    "    \"\"\"))\n",
    "\n",
    "# Quick peek\n",
    "import pandas as pd\n",
    "with engine.connect() as conn:\n",
    "    sample = pd.read_sql(\"\"\"\n",
    "        SELECT video_id, title, trending_date, views, publish_time\n",
    "        FROM gb_videos\n",
    "        ORDER BY publish_time DESC NULLS LAST\n",
    "        LIMIT 5\n",
    "    \"\"\", conn)\n",
    "    display(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb8dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique trending dates (batches): 205\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>2017-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>2017-11-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>2017-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>2017-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>2017-11-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trending_date_raw   parsed_dt\n",
       "0        2017-11-14  2017-11-14\n",
       "1        2017-11-15  2017-11-15\n",
       "2        2017-11-16  2017-11-16\n",
       "3        2017-11-17  2017-11-17\n",
       "4        2017-11-18  2017-11-18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- imports & engine ---\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "EXTRACT_RUN_ID = datetime.now(timezone.utc).strftime(\"yt_gb_%Y%m%dT%H%M%S\")\n",
    "\n",
    "# --- Step A: enumerate & order unique dates deterministically ---\n",
    "SQL_ENUM = text(\"\"\"\n",
    "SELECT\n",
    "  trending_date AS trending_date_raw,\n",
    "  trending_date AS parsed_dt\n",
    "FROM gb_videos\n",
    "WHERE trending_date IS NOT NULL\n",
    "GROUP BY trending_date\n",
    "ORDER BY trending_date;\n",
    "\"\"\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    dates_df = pd.read_sql(SQL_ENUM, conn)\n",
    "\n",
    "num_batches = len(dates_df)\n",
    "print(f\"Unique trending dates (batches): {num_batches}\")\n",
    "display(dates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784e1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step B: batch extractor (generator) ---\n",
    "def iter_daily_batches(conn, dates_df):\n",
    "    \"\"\"\n",
    "    Yields (batch_id, trending_date_raw, parsed_dt, dataframe_for_that_day)\n",
    "    \"\"\"\n",
    "    for i, row in dates_df.reset_index(drop=True).iterrows():\n",
    "        batch_id = i + 1\n",
    "        t_raw = row[\"trending_date_raw\"]\n",
    "        p_dt = row[\"parsed_dt\"]  # may be NaT if unparsable; fine for extract\n",
    "\n",
    "        # fetch the day's rows using the RAW value (no transform here)\n",
    "        df_day = pd.read_sql(\n",
    "            text(\"\"\"\n",
    "                SELECT *\n",
    "                FROM gb_videos\n",
    "                WHERE trending_date = :t_raw\n",
    "                ORDER BY video_id, trending_date\n",
    "            \"\"\"),\n",
    "            conn,\n",
    "            params={\"t_raw\": t_raw},\n",
    "        )\n",
    "        yield batch_id, t_raw, p_dt, df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ba6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS extract_day_batches (\n",
    "            id BIGSERIAL PRIMARY KEY,\n",
    "            extract_run_id TEXT NOT NULL,\n",
    "            batch_id INT NOT NULL,\n",
    "            trending_date_raw DATE,\n",
    "            parsed_dt DATE,\n",
    "            row_count INT,\n",
    "            created_at TIMESTAMPTZ DEFAULT NOW()\n",
    "        );\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26c1bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract run id: yt_gb_20251020T192222\n",
      "Total rows extracted across all day-batches: 38742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step C: run the extract, optionally log metadata ---\n",
    "total_rows = 0\n",
    "batch_rows = []\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    for batch_id, t_raw, p_dt, df_day in iter_daily_batches(conn, dates_df):\n",
    "        out_path = f\"extract/day={t_raw}/gb_videos_day_{t_raw}.csv\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        df_day.to_csv(out_path, index=False)\n",
    "\n",
    "        # record metadata\n",
    "        rc = len(df_day)\n",
    "        total_rows += rc\n",
    "        batch_rows.append({\"batch_id\": batch_id, \"trending_date_raw\": t_raw, \"parsed_dt\": p_dt, \"row_count\": rc})\n",
    "\n",
    "        # persist metadata\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO extract_day_batches (extract_run_id, batch_id, trending_date_raw, parsed_dt, row_count)\n",
    "                VALUES (:rid, :bid, :traw, :pdt, :rc)\n",
    "            \"\"\"),\n",
    "            {\"rid\": EXTRACT_RUN_ID, \"bid\": batch_id, \"traw\": t_raw, \"pdt\": p_dt if pd.notna(p_dt) else None, \"rc\": rc}\n",
    "        )\n",
    "\n",
    "print(f\"Extract run id: {EXTRACT_RUN_ID}\")\n",
    "print(f\"Total rows extracted across all day-batches: {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6603a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCEPTANCE CHECKS\n",
      "✅ Unique trending dates listed: 205 (db says 205)\n",
      "✅ Expected number of batches: 205\n",
      "✅ Sum(rows in all batches) vs total rows in main table: 38742 vs 38742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id trending_date_raw   parsed_dt  row_count\n",
       "0         1        2017-11-14  2017-11-14        200\n",
       "1         2        2017-11-15  2017-11-15        199\n",
       "2         3        2017-11-16  2017-11-16        199\n",
       "3         4        2017-11-17  2017-11-17        200\n",
       "4         5        2017-11-18  2017-11-18        200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>201</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>202</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>203</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>204</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>205</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch_id trending_date_raw   parsed_dt  row_count\n",
       "200       201        2018-06-10  2018-06-10        168\n",
       "201       202        2018-06-11  2018-06-11        166\n",
       "202       203        2018-06-12  2018-06-12        171\n",
       "203       204        2018-06-13  2018-06-13        170\n",
       "204       205        2018-06-14  2018-06-14        165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-batch row count sum: 38742\n"
     ]
    }
   ],
   "source": [
    "# --- Step D: acceptance checks ---\n",
    "with engine.begin() as conn:\n",
    "    src_total = conn.execute(text(\"SELECT COUNT(*) FROM gb_videos\")).scalar()\n",
    "    uniq_dates = conn.execute(text(\"SELECT COUNT(DISTINCT trending_date) FROM gb_videos WHERE trending_date IS NOT NULL\")).scalar()\n",
    "\n",
    "print(\"ACCEPTANCE CHECKS\")\n",
    "print(f\"✅ Unique trending dates listed: {num_batches} (db says {uniq_dates})\")\n",
    "print(f\"✅ Expected number of batches: {num_batches}\")\n",
    "print(f\"✅ Sum(rows in all batches) vs total rows in main table: {total_rows} vs {src_total}\")\n",
    "\n",
    "# (Optional) Inspect the logged batches quickly in Python:\n",
    "meta_df = pd.DataFrame(batch_rows)\n",
    "meta_df.sort_values(\"batch_id\", inplace=True)\n",
    "display(meta_df.head())\n",
    "display(meta_df.tail())\n",
    "print(\"Per-batch row count sum:\", meta_df[\"row_count\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa80758",
   "metadata": {},
   "source": [
    "##Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88fb243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample function to extract date\n",
    "def extract_date(datetime_str):\n",
    "    return pd.to_datetime(datetime_str).date()\n",
    "\n",
    "# Sample function to extract time\n",
    "def extract_time(datetime_str):\n",
    "    return pd.to_datetime(datetime_str).time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e8d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed batch saved: transform/day=2017-11-14.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-15.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-16.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed batch saved: transform/day=2017-11-17.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-18.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-19.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-20.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-21.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-22.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-23.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-24.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-25.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-26.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-27.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-28.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-29.csv\n",
      "✅ Transformed batch saved: transform/day=2017-11-30.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-01.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-02.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-03.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-04.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-05.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-06.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-07.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-08.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-09.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-10.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-11.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-12.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-13.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-14.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-15.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-16.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-17.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-18.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-19.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-20.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-21.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-22.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-23.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-24.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-25.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-26.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-27.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-28.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-29.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-30.csv\n",
      "✅ Transformed batch saved: transform/day=2017-12-31.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-08.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-09.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-12.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-13.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-14.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-15.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-16.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-17.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-18.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-19.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-20.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-21.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-22.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-23.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-24.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-25.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-26.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-27.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-28.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-29.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-30.csv\n",
      "✅ Transformed batch saved: transform/day=2018-01-31.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-08.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-09.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-10.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-11.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-12.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-13.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-14.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-15.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-16.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-17.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-18.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-19.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-20.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-21.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-22.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-23.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-24.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-25.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-26.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-27.csv\n",
      "✅ Transformed batch saved: transform/day=2018-02-28.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-08.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-09.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-10.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-11.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-12.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-13.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-14.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-15.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-16.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-17.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-18.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-19.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-20.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-21.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-22.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-23.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-24.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-25.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-26.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-27.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-28.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-29.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-30.csv\n",
      "✅ Transformed batch saved: transform/day=2018-03-31.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-14.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-15.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-16.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-17.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-18.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-19.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-20.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-21.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-22.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-23.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-24.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-25.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-26.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-27.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-28.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-29.csv\n",
      "✅ Transformed batch saved: transform/day=2018-04-30.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-08.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-09.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-10.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-11.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-12.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-13.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-14.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-15.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-16.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-17.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-18.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-19.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-20.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-21.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-22.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-23.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-24.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-25.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-26.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-27.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-28.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-29.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-30.csv\n",
      "✅ Transformed batch saved: transform/day=2018-05-31.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-01.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-02.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-03.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-04.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-05.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-06.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-07.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-08.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-09.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-10.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-11.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-12.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-13.csv\n",
      "✅ Transformed batch saved: transform/day=2018-06-14.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "extract_root = Path(\"extract\")\n",
    "transform_root = Path(\"transform\")\n",
    "transform_root.mkdir(exist_ok=True)\n",
    "\n",
    "def split_music_title(title):\n",
    "    for delim in [\" - \", \"|\"]:\n",
    "        if delim in title:\n",
    "            parts = title.split(delim, 1)\n",
    "            return parts[0].strip(), parts[1].strip()\n",
    "    return None, title\n",
    "\n",
    "for day_folder in sorted(extract_root.glob(\"day=*\")):\n",
    "    date_str = day_folder.name.split(\"=\")[1]\n",
    "    csv_files = list(day_folder.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️ No CSV found in {day_folder}\")\n",
    "        continue\n",
    "\n",
    "    raw_batch_df = pd.read_csv(csv_files[0])\n",
    "\n",
    "    # --- Title & Channel Logic ---\n",
    "    for idx, r in raw_batch_df.iterrows():\n",
    "        if r[\"category_id\"] == 10:  # Music\n",
    "            artist, title = split_music_title(r[\"title\"])\n",
    "            raw_batch_df.loc[idx, \"Artist\"] = artist or r[\"channel_title\"]\n",
    "            raw_batch_df.loc[idx, \"Title\"] = title\n",
    "        elif r[\"category_id\"] == 23:  # Celebrity Parody\n",
    "            if \"|\" in r[\"title\"]:\n",
    "                parts = r[\"title\"].split(\"|\", 1)\n",
    "                raw_batch_df.loc[idx, \"Author\"] = parts[0].strip()\n",
    "                raw_batch_df.loc[idx, \"Title\"] = parts[1].strip()\n",
    "            else:\n",
    "                raw_batch_df.loc[idx, \"Author\"] = r[\"channel_title\"]\n",
    "                raw_batch_df.loc[idx, \"Title\"] = r[\"title\"]\n",
    "        else:  # Others\n",
    "            raw_batch_df.loc[idx, \"Author\"] = r[\"channel_title\"]\n",
    "            raw_batch_df.loc[idx, \"Title\"] = r[\"title\"]\n",
    "\n",
    "    # --- Date Standardization ---\n",
    "    raw_batch_df[\"trending_date\"] = pd.to_datetime(raw_batch_df[\"trending_date\"]).dt.date\n",
    "    raw_batch_df[\"publish_date\"] = pd.to_datetime(raw_batch_df[\"publish_time\"]).dt.date\n",
    "    raw_batch_df[\"publish_time\"] = pd.to_datetime(raw_batch_df[\"publish_time\"]).dt.time\n",
    "\n",
    "    # --- Drop Columns ---\n",
    "    cols_to_drop = [\n",
    "        \"tags\", \"thumbnail_link\", \"ratings_disabled\",\n",
    "        \"video_error_or_removed\", \"comments_disabled\", \"description\", \"title\", \"channel_title\"\n",
    "    ]\n",
    "    raw_batch_df.drop(columns=[c for c in cols_to_drop if c in raw_batch_df.columns], inplace=True)\n",
    "\n",
    "    # --- Save ---\n",
    "    out_path = transform_root / f\"day={date_str}.csv\"\n",
    "    raw_batch_df.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Transformed batch saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3757072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'video_id', 'trending_date', 'category_id', 'publish_time',\n",
      "       'views', 'likes', 'dislikes', 'comment_count', 'Author', 'Title',\n",
      "       'Artist', 'publish_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_batch_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d9a96bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Issues found in batch 2018-06-14:\n",
      "   - 1 rows have invalid trending_date format.\n"
     ]
    }
   ],
   "source": [
    "## tests\n",
    "pre_row_count = len(raw_batch_df)\n",
    "pre_cols = set(raw_batch_df.columns)\n",
    "\n",
    "# --- Acceptance Checks ---\n",
    "issues = []\n",
    "\n",
    "# 1️⃣ Row count consistency\n",
    "post_row_count = len(raw_batch_df)\n",
    "if pre_row_count != post_row_count:\n",
    "    issues.append(f\"Row count mismatch: before={pre_row_count}, after={post_row_count}\")\n",
    "\n",
    "# 2️⃣ Music split validation\n",
    "if \"Artist\" in raw_batch_df.columns:\n",
    "    invalid_music = raw_batch_df[\n",
    "        (raw_batch_df[\"category_id\"] == 10) & (raw_batch_df[\"Artist\"].isna())\n",
    "    ]\n",
    "    if not invalid_music.empty:\n",
    "        issues.append(f\"{len(invalid_music)} music rows missing Artist field.\")\n",
    "\n",
    "# 3️⃣ Category 23 validation\n",
    "if \"Author\" in raw_batch_df.columns:\n",
    "    invalid_cat23 = raw_batch_df[\n",
    "        (raw_batch_df[\"category_id\"] == 23) & (raw_batch_df[\"Author\"].isna())\n",
    "    ]\n",
    "    if not invalid_cat23.empty:\n",
    "        issues.append(f\"{len(invalid_cat23)} parody rows missing Author field.\")\n",
    "\n",
    "# 4️⃣ Trending date ISO check\n",
    "invalid_dates = raw_batch_df[\n",
    "    ~raw_batch_df[\"trending_date\"].astype(str).str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "]\n",
    "if not invalid_dates.empty:\n",
    "    issues.append(f\"{len(invalid_dates)} rows have invalid trending_date format.\")\n",
    "\n",
    "# 5️⃣ Publish date/time columns exist\n",
    "missing_cols = [c for c in [\"publish_date\", \"publish_time\"] if c not in raw_batch_df.columns]\n",
    "if missing_cols:\n",
    "    issues.append(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "# 6️⃣ Dropped columns check\n",
    "expected_drops = {\n",
    "    \"tags\", \"thumbnail_link\", \"ratings_disabled\",\n",
    "    \"video_error_or_removed\", \"comments_disabled\", \"description\"\n",
    "}\n",
    "still_there = pre_cols.intersection(raw_batch_df.columns).intersection(expected_drops)\n",
    "if still_there:\n",
    "    issues.append(f\"Columns not dropped: {still_there}\")\n",
    "\n",
    "# --- Report Results ---\n",
    "if issues:\n",
    "    print(f\"⚠️ Issues found in batch {date_str}:\")\n",
    "    for i in issues:\n",
    "        print(f\"   - {i}\")\n",
    "else:\n",
    "    print(f\"✅ Batch {date_str} passed all acceptance checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b08c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = transform_root / f\"day={date_str}_validation.txt\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    if issues:\n",
    "        f.write(f\"⚠️ Issues in batch {date_str}:\\n\")\n",
    "        for i in issues:\n",
    "            f.write(f\" - {i}\\n\")\n",
    "    else:\n",
    "        f.write(f\"✅ Batch {date_str} passed all checks.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c9a51",
   "metadata": {},
   "source": [
    "##LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc2b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories in batch: [24.  1. 10. 23. 17. 20. 26. 25. 22. 15. nan 43. 27.]\n"
     ]
    }
   ],
   "source": [
    "cat = raw_batch_df['category_id'].unique()\n",
    "print(\"Unique categories in batch:\", cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
