{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51534c19",
   "metadata": {},
   "source": [
    "##Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cba0ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.0.43)\n",
      "Requirement already satisfied: psycopg2-binary in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.12/site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from SQLAlchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from SQLAlchemy) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install SQLAlchemy psycopg2-binary pandas python-dateutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e32a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL 16.10 (Ubuntu 16.10-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "PG_USER = \"postgres\"\n",
    "PG_PWD  = \"postgres\"\n",
    "PG_HOST = \"127.0.0.1\"\n",
    "PG_PORT = 5432\n",
    "DB_NAME = \"4260354_gb_youtube_trends\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{PG_USER}:{PG_PWD}@{PG_HOST}:{PG_PORT}/{DB_NAME}\")\n",
    "with engine.connect() as conn:\n",
    "    print(conn.execute(text(\"select version()\")).scalar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4866a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS gb_videos CASCADE;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b8de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exists? True GBvideos.csv\n",
      "Raw rows: 38916 | columns: ['video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description']\n",
      "Parsed trending_date non-null: 38916 of 38916\n",
      "After dropna on keys: 38916 (dropped 0)\n",
      "After de-dup keys: 38742\n",
      "Inserted rows: 38742\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "from sqlalchemy import text, Text, Integer, BigInteger, Boolean, Date, DateTime\n",
    "\n",
    "csv_path = \"GBvideos.csv\"  # <-- set correctly\n",
    "print(\"CSV exists?\", os.path.exists(csv_path), csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(\"Raw rows:\", len(df), \"| columns:\", list(df.columns))\n",
    "\n",
    "# Robust date parsing (handles multiple common formats)\n",
    "def parse_trending_series(s):\n",
    "    c = pd.to_datetime(s, format=\"%y.%d.%m\", errors=\"coerce\")\n",
    "    if c.notna().sum() == 0:\n",
    "        c = pd.to_datetime(s, format=\"%y.%m.%d\", errors=\"coerce\")\n",
    "    if c.notna().sum() == 0:\n",
    "        c = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return c\n",
    "\n",
    "df[\"trending_date\"] = parse_trending_series(df[\"trending_date\"]).dt.date\n",
    "if \"publish_time\" in df.columns:\n",
    "    df[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "for col in [\"comments_disabled\",\"ratings_disabled\",\"video_error_or_removed\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "for col in [\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_id\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Parsed trending_date non-null:\", df[\"trending_date\"].notna().sum(), \"of\", len(df))\n",
    "\n",
    "# Ensure table exists (no unique index yet to avoid conflicts on first load)\n",
    "ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gb_videos (\n",
    "  id BIGSERIAL PRIMARY KEY,\n",
    "  video_id TEXT NOT NULL,\n",
    "  trending_date DATE NOT NULL,\n",
    "  title TEXT,\n",
    "  channel_title TEXT,\n",
    "  category_id INT,\n",
    "  publish_time TIMESTAMPTZ,\n",
    "  tags TEXT,\n",
    "  views BIGINT,\n",
    "  likes BIGINT,\n",
    "  dislikes BIGINT,\n",
    "  comment_count BIGINT,\n",
    "  thumbnail_link TEXT,\n",
    "  comments_disabled BOOLEAN,\n",
    "  ratings_disabled BOOLEAN,\n",
    "  video_error_or_removed BOOLEAN,\n",
    "  description TEXT\n",
    ");\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(ddl))\n",
    "\n",
    "# Filter to known columns and drop empties on key fields\n",
    "dtype_map = {\n",
    "    \"video_id\": Text(),\n",
    "    \"trending_date\": Date(),\n",
    "    \"title\": Text(),\n",
    "    \"channel_title\": Text(),\n",
    "    \"category_id\": Integer(),\n",
    "    \"publish_time\": DateTime(timezone=True),\n",
    "    \"tags\": Text(),\n",
    "    \"views\": BigInteger(),\n",
    "    \"likes\": BigInteger(),\n",
    "    \"dislikes\": BigInteger(),\n",
    "    \"comment_count\": BigInteger(),\n",
    "    \"thumbnail_link\": Text(),\n",
    "    \"comments_disabled\": Boolean(),\n",
    "    \"ratings_disabled\": Boolean(),\n",
    "    \"video_error_or_removed\": Boolean(),\n",
    "    \"description\": Text(),\n",
    "}\n",
    "\n",
    "cols = [c for c in dtype_map if c in df.columns]\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"video_id\",\"trending_date\"])\n",
    "print(\"After dropna on keys:\", len(df), f\"(dropped {before-len(df)})\")\n",
    "df = df.drop_duplicates(subset=[\"video_id\",\"trending_date\"])\n",
    "print(\"After de-dup keys:\", len(df))\n",
    "\n",
    "df[cols].to_sql(\n",
    "    \"gb_videos\",\n",
    "    engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    dtype=dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=10000,\n",
    ")\n",
    "print(\"Inserted rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796e97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count now: 38742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>views</th>\n",
       "      <th>publish_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r63VBOagGAo</td>\n",
       "      <td>Shawn Mendes x Portugal (FPF Official World Cu...</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>653114</td>\n",
       "      <td>2018-06-13 13:11:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YQJmvXamKYg</td>\n",
       "      <td>Conway: People are bending to the will of Pres...</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>99048</td>\n",
       "      <td>2018-06-13 12:56:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-QPdRfqTnt4</td>\n",
       "      <td>Dumbo Official Teaser Trailer</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>4427381</td>\n",
       "      <td>2018-06-13 07:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6h8QgZF5Qu4</td>\n",
       "      <td>Drop the Mic w/ Ashton Kutcher &amp; Sean Diddy Combs</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>864189</td>\n",
       "      <td>2018-06-13 05:27:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arY6lepNdzU</td>\n",
       "      <td>E3 2018 Exclusive Gameplay Demos, Interviews a...</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>349122</td>\n",
       "      <td>2018-06-13 04:09:23+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  r63VBOagGAo  Shawn Mendes x Portugal (FPF Official World Cu...   \n",
       "1  YQJmvXamKYg  Conway: People are bending to the will of Pres...   \n",
       "2  -QPdRfqTnt4                      Dumbo Official Teaser Trailer   \n",
       "3  6h8QgZF5Qu4  Drop the Mic w/ Ashton Kutcher & Sean Diddy Combs   \n",
       "4  arY6lepNdzU  E3 2018 Exclusive Gameplay Demos, Interviews a...   \n",
       "\n",
       "  trending_date    views              publish_time  \n",
       "0    2018-06-14   653114 2018-06-13 13:11:56+00:00  \n",
       "1    2018-06-14    99048 2018-06-13 12:56:49+00:00  \n",
       "2    2018-06-14  4427381 2018-06-13 07:00:00+00:00  \n",
       "3    2018-06-14   864189 2018-06-13 05:27:27+00:00  \n",
       "4    2018-06-13   349122 2018-06-13 04:09:23+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.begin() as conn:\n",
    "    n = conn.execute(text(\"SELECT COUNT(*) FROM gb_videos\")).scalar()\n",
    "    print(\"Row count now:\", n)\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE UNIQUE INDEX IF NOT EXISTS ux_gb_videos_vid_trend\n",
    "          ON gb_videos (video_id, trending_date);\n",
    "        CREATE INDEX IF NOT EXISTS ix_gb_videos_category_id ON gb_videos (category_id);\n",
    "        CREATE INDEX IF NOT EXISTS ix_gb_videos_publish_time ON gb_videos (publish_time);\n",
    "    \"\"\"))\n",
    "\n",
    "# Quick peek\n",
    "import pandas as pd\n",
    "with engine.connect() as conn:\n",
    "    sample = pd.read_sql(\"\"\"\n",
    "        SELECT video_id, title, trending_date, views, publish_time\n",
    "        FROM gb_videos\n",
    "        ORDER BY publish_time DESC NULLS LAST\n",
    "        LIMIT 5\n",
    "    \"\"\", conn)\n",
    "    display(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb8dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique trending dates (batches): 205\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>2017-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>2017-11-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>2017-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>2017-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>2017-11-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trending_date_raw   parsed_dt\n",
       "0        2017-11-14  2017-11-14\n",
       "1        2017-11-15  2017-11-15\n",
       "2        2017-11-16  2017-11-16\n",
       "3        2017-11-17  2017-11-17\n",
       "4        2017-11-18  2017-11-18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- imports & engine ---\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "EXTRACT_RUN_ID = datetime.now(timezone.utc).strftime(\"yt_gb_%Y%m%dT%H%M%S\")\n",
    "\n",
    "# --- Step A: enumerate & order unique dates deterministically ---\n",
    "SQL_ENUM = text(\"\"\"\n",
    "SELECT\n",
    "  trending_date AS trending_date_raw,\n",
    "  trending_date AS parsed_dt\n",
    "FROM gb_videos\n",
    "WHERE trending_date IS NOT NULL\n",
    "GROUP BY trending_date\n",
    "ORDER BY trending_date;\n",
    "\"\"\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    dates_df = pd.read_sql(SQL_ENUM, conn)\n",
    "\n",
    "num_batches = len(dates_df)\n",
    "print(f\"Unique trending dates (batches): {num_batches}\")\n",
    "display(dates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "784e1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step B: batch extractor (generator) ---\n",
    "def iter_daily_batches(conn, dates_df):\n",
    "    \"\"\"\n",
    "    Yields (batch_id, trending_date_raw, parsed_dt, dataframe_for_that_day)\n",
    "    \"\"\"\n",
    "    for i, row in dates_df.reset_index(drop=True).iterrows():\n",
    "        batch_id = i + 1\n",
    "        t_raw = row[\"trending_date_raw\"]\n",
    "        p_dt = row[\"parsed_dt\"]  # may be NaT if unparsable; fine for extract\n",
    "\n",
    "        # fetch the day's rows using the RAW value (no transform here)\n",
    "        df_day = pd.read_sql(\n",
    "            text(\"\"\"\n",
    "                SELECT *\n",
    "                FROM gb_videos\n",
    "                WHERE trending_date = :t_raw\n",
    "                ORDER BY video_id, trending_date\n",
    "            \"\"\"),\n",
    "            conn,\n",
    "            params={\"t_raw\": t_raw},\n",
    "        )\n",
    "        yield batch_id, t_raw, p_dt, df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ba6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS extract_day_batches (\n",
    "            id BIGSERIAL PRIMARY KEY,\n",
    "            extract_run_id TEXT NOT NULL,\n",
    "            batch_id INT NOT NULL,\n",
    "            trending_date_raw DATE,\n",
    "            parsed_dt DATE,\n",
    "            row_count INT,\n",
    "            created_at TIMESTAMPTZ DEFAULT NOW()\n",
    "        );\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c1bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract run id: yt_gb_20251007T075408\n",
      "Total rows extracted across all day-batches: 38742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step C: run the extract, optionally log metadata ---\n",
    "total_rows = 0\n",
    "batch_rows = []\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    for batch_id, t_raw, p_dt, df_day in iter_daily_batches(conn, dates_df):\n",
    "        out_path = f\"extract/day={t_raw}/gb_videos_day_{t_raw}.csv\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        df_day.to_csv(out_path, index=False)\n",
    "\n",
    "        # record metadata\n",
    "        rc = len(df_day)\n",
    "        total_rows += rc\n",
    "        batch_rows.append({\"batch_id\": batch_id, \"trending_date_raw\": t_raw, \"parsed_dt\": p_dt, \"row_count\": rc})\n",
    "\n",
    "        # persist metadata\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO extract_day_batches (extract_run_id, batch_id, trending_date_raw, parsed_dt, row_count)\n",
    "                VALUES (:rid, :bid, :traw, :pdt, :rc)\n",
    "            \"\"\"),\n",
    "            {\"rid\": EXTRACT_RUN_ID, \"bid\": batch_id, \"traw\": t_raw, \"pdt\": p_dt if pd.notna(p_dt) else None, \"rc\": rc}\n",
    "        )\n",
    "\n",
    "print(f\"Extract run id: {EXTRACT_RUN_ID}\")\n",
    "print(f\"Total rows extracted across all day-batches: {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6603a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCEPTANCE CHECKS\n",
      "✅ Unique trending dates listed: 205 (db says 205)\n",
      "✅ Expected number of batches: 205\n",
      "✅ Sum(rows in all batches) vs total rows in main table: 38742 vs 38742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id trending_date_raw   parsed_dt  row_count\n",
       "0         1        2017-11-14  2017-11-14        200\n",
       "1         2        2017-11-15  2017-11-15        199\n",
       "2         3        2017-11-16  2017-11-16        199\n",
       "3         4        2017-11-17  2017-11-17        200\n",
       "4         5        2017-11-18  2017-11-18        200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>trending_date_raw</th>\n",
       "      <th>parsed_dt</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>201</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>202</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>203</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>204</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>205</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch_id trending_date_raw   parsed_dt  row_count\n",
       "200       201        2018-06-10  2018-06-10        168\n",
       "201       202        2018-06-11  2018-06-11        166\n",
       "202       203        2018-06-12  2018-06-12        171\n",
       "203       204        2018-06-13  2018-06-13        170\n",
       "204       205        2018-06-14  2018-06-14        165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-batch row count sum: 38742\n"
     ]
    }
   ],
   "source": [
    "# --- Step D: acceptance checks ---\n",
    "with engine.begin() as conn:\n",
    "    src_total = conn.execute(text(\"SELECT COUNT(*) FROM gb_videos\")).scalar()\n",
    "    uniq_dates = conn.execute(text(\"SELECT COUNT(DISTINCT trending_date) FROM gb_videos WHERE trending_date IS NOT NULL\")).scalar()\n",
    "\n",
    "print(\"ACCEPTANCE CHECKS\")\n",
    "print(f\"✅ Unique trending dates listed: {num_batches} (db says {uniq_dates})\")\n",
    "print(f\"✅ Expected number of batches: {num_batches}\")\n",
    "print(f\"✅ Sum(rows in all batches) vs total rows in main table: {total_rows} vs {src_total}\")\n",
    "\n",
    "# (Optional) Inspect the logged batches quickly in Python:\n",
    "meta_df = pd.DataFrame(batch_rows)\n",
    "meta_df.sort_values(\"batch_id\", inplace=True)\n",
    "display(meta_df.head())\n",
    "display(meta_df.tail())\n",
    "print(\"Per-batch row count sum:\", meta_df[\"row_count\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa80758",
   "metadata": {},
   "source": [
    "##Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fb243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample function to extract date\n",
    "def extract_date(datetime_str):\n",
    "    return pd.to_datetime(datetime_str).date()\n",
    "\n",
    "# Sample function to extract time\n",
    "def extract_time(datetime_str):\n",
    "    return pd.to_datetime(datetime_str).time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e8d80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m extract_root = Path(\u001b[33m\"\u001b[39m\u001b[33mextract\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     51\u001b[39m     ArrowDtype,\n\u001b[32m     52\u001b[39m     Int8Dtype,\n\u001b[32m     53\u001b[39m     Int16Dtype,\n\u001b[32m     54\u001b[39m     Int32Dtype,\n\u001b[32m     55\u001b[39m     Int64Dtype,\n\u001b[32m     56\u001b[39m     UInt8Dtype,\n\u001b[32m     57\u001b[39m     UInt16Dtype,\n\u001b[32m     58\u001b[39m     UInt32Dtype,\n\u001b[32m     59\u001b[39m     UInt64Dtype,\n\u001b[32m     60\u001b[39m     Float32Dtype,\n\u001b[32m     61\u001b[39m     Float64Dtype,\n\u001b[32m     62\u001b[39m     CategoricalDtype,\n\u001b[32m     63\u001b[39m     PeriodDtype,\n\u001b[32m     64\u001b[39m     IntervalDtype,\n\u001b[32m     65\u001b[39m     DatetimeTZDtype,\n\u001b[32m     66\u001b[39m     StringDtype,\n\u001b[32m     67\u001b[39m     BooleanDtype,\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     69\u001b[39m     NA,\n\u001b[32m     70\u001b[39m     isna,\n\u001b[32m     71\u001b[39m     isnull,\n\u001b[32m     72\u001b[39m     notna,\n\u001b[32m     73\u001b[39m     notnull,\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     75\u001b[39m     Index,\n\u001b[32m     76\u001b[39m     CategoricalIndex,\n\u001b[32m     77\u001b[39m     RangeIndex,\n\u001b[32m     78\u001b[39m     MultiIndex,\n\u001b[32m     79\u001b[39m     IntervalIndex,\n\u001b[32m     80\u001b[39m     TimedeltaIndex,\n\u001b[32m     81\u001b[39m     DatetimeIndex,\n\u001b[32m     82\u001b[39m     PeriodIndex,\n\u001b[32m     83\u001b[39m     IndexSlice,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     85\u001b[39m     NaT,\n\u001b[32m     86\u001b[39m     Period,\n\u001b[32m     87\u001b[39m     period_range,\n\u001b[32m     88\u001b[39m     Timedelta,\n\u001b[32m     89\u001b[39m     timedelta_range,\n\u001b[32m     90\u001b[39m     Timestamp,\n\u001b[32m     91\u001b[39m     date_range,\n\u001b[32m     92\u001b[39m     bdate_range,\n\u001b[32m     93\u001b[39m     Interval,\n\u001b[32m     94\u001b[39m     interval_range,\n\u001b[32m     95\u001b[39m     DateOffset,\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m     97\u001b[39m     to_numeric,\n\u001b[32m     98\u001b[39m     to_datetime,\n\u001b[32m     99\u001b[39m     to_timedelta,\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    101\u001b[39m     Flags,\n\u001b[32m    102\u001b[39m     Grouper,\n\u001b[32m    103\u001b[39m     factorize,\n\u001b[32m    104\u001b[39m     unique,\n\u001b[32m    105\u001b[39m     value_counts,\n\u001b[32m    106\u001b[39m     NamedAgg,\n\u001b[32m    107\u001b[39m     array,\n\u001b[32m    108\u001b[39m     Categorical,\n\u001b[32m    109\u001b[39m     set_eng_float_format,\n\u001b[32m    110\u001b[39m     Series,\n\u001b[32m    111\u001b[39m     DataFrame,\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/api.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     Grouper,\n\u001b[32m     49\u001b[39m     NamedAgg,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     CategoricalIndex,\n\u001b[32m     53\u001b[39m     DatetimeIndex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     TimedeltaIndex,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     bdate_range,\n\u001b[32m     63\u001b[39m     date_range,\n\u001b[32m     64\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     DataFrameGroupBy,\n\u001b[32m      3\u001b[39m     NamedAgg,\n\u001b[32m      4\u001b[39m     SeriesGroupBy,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/generic.py:60\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     55\u001b[39m     isna,\n\u001b[32m     56\u001b[39m     notna,\n\u001b[32m     57\u001b[39m )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     61\u001b[39m     GroupByApply,\n\u001b[32m     62\u001b[39m     maybe_mangle_lambdas,\n\u001b[32m     63\u001b[39m     reconstruct_func,\n\u001b[32m     64\u001b[39m     validate_func_kwargs,\n\u001b[32m     65\u001b[39m     warn_alias_replacement,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcom\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:990\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1086\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1186\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "extract_root = Path(\"extract\")\n",
    "transform_root = Path(\"transform\")\n",
    "transform_root.mkdir(exist_ok=True)\n",
    "\n",
    "def split_music_title(title):\n",
    "    for delim in [\" - \", \"|\"]:\n",
    "        if delim in title:\n",
    "            parts = title.split(delim, 1)\n",
    "            return parts[0].strip(), parts[1].strip()\n",
    "    return None, title\n",
    "\n",
    "for day_folder in sorted(extract_root.glob(\"day=*\")):\n",
    "    date_str = day_folder.name.split(\"=\")[1]\n",
    "    csv_files = list(day_folder.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️ No CSV found in {day_folder}\")\n",
    "        continue\n",
    "\n",
    "    raw_batch_df = pd.read_csv(csv_files[0])\n",
    "\n",
    "    # --- Title & Channel Logic ---\n",
    "    for idx, r in raw_batch_df.iterrows():\n",
    "        if r[\"category_id\"] == 10:  # Music\n",
    "            artist, title = split_music_title(r[\"title\"])\n",
    "            raw_batch_df.loc[idx, \"Artist\"] = artist or r[\"channel_title\"]\n",
    "            raw_batch_df.loc[idx, \"Title\"] = title\n",
    "        elif r[\"category_id\"] == 23:  # Celebrity Parody\n",
    "            if \"|\" in r[\"title\"]:\n",
    "                parts = r[\"title\"].split(\"|\", 1)\n",
    "                raw_batch_df.loc[idx, \"Author\"] = parts[0].strip()\n",
    "                raw_batch_df.loc[idx, \"Title\"] = parts[1].strip()\n",
    "            else:\n",
    "                raw_batch_df.loc[idx, \"Author\"] = r[\"channel_title\"]\n",
    "                raw_batch_df.loc[idx, \"Title\"] = r[\"title\"]\n",
    "        else:  # Others\n",
    "            raw_batch_df.loc[idx, \"Author\"] = r[\"channel_title\"]\n",
    "            raw_batch_df.loc[idx, \"Title\"] = r[\"title\"]\n",
    "\n",
    "    # --- Date Standardization ---\n",
    "    raw_batch_df[\"trending_date\"] = pd.to_datetime(raw_batch_df[\"trending_date\"]).dt.date\n",
    "    raw_batch_df[\"publish_date\"] = pd.to_datetime(raw_batch_df[\"publish_time\"]).dt.date\n",
    "    raw_batch_df[\"publish_time\"] = pd.to_datetime(raw_batch_df[\"publish_time\"]).dt.time\n",
    "\n",
    "    # --- Drop Columns ---\n",
    "    cols_to_drop = [\n",
    "        \"tags\", \"thumbnail_link\", \"ratings_disabled\",\n",
    "        \"video_error_or_removed\", \"comments_disabled\", \"description\", \"title\", \"channel_title\"\n",
    "    ]\n",
    "    raw_batch_df.drop(columns=[c for c in cols_to_drop if c in raw_batch_df.columns], inplace=True)\n",
    "\n",
    "    # --- Save ---\n",
    "    out_path = transform_root / f\"day={date_str}.csv\"\n",
    "    raw_batch_df.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Transformed batch saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3757072e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_batch_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mraw_batch_df\u001b[49m.columns)\n",
      "\u001b[31mNameError\u001b[39m: name 'raw_batch_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(raw_batch_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9a96bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Issues found in batch 2018-06-14:\n",
      "   - 1 rows have invalid trending_date format.\n"
     ]
    }
   ],
   "source": [
    "## tests\n",
    "pre_row_count = len(raw_batch_df)\n",
    "pre_cols = set(raw_batch_df.columns)\n",
    "\n",
    "# --- Acceptance Checks ---\n",
    "issues = []\n",
    "\n",
    "# 1️⃣ Row count consistency\n",
    "post_row_count = len(raw_batch_df)\n",
    "if pre_row_count != post_row_count:\n",
    "    issues.append(f\"Row count mismatch: before={pre_row_count}, after={post_row_count}\")\n",
    "\n",
    "# 2️⃣ Music split validation\n",
    "if \"Artist\" in raw_batch_df.columns:\n",
    "    invalid_music = raw_batch_df[\n",
    "        (raw_batch_df[\"category_id\"] == 10) & (raw_batch_df[\"Artist\"].isna())\n",
    "    ]\n",
    "    if not invalid_music.empty:\n",
    "        issues.append(f\"{len(invalid_music)} music rows missing Artist field.\")\n",
    "\n",
    "# 3️⃣ Category 23 validation\n",
    "if \"Author\" in raw_batch_df.columns:\n",
    "    invalid_cat23 = raw_batch_df[\n",
    "        (raw_batch_df[\"category_id\"] == 23) & (raw_batch_df[\"Author\"].isna())\n",
    "    ]\n",
    "    if not invalid_cat23.empty:\n",
    "        issues.append(f\"{len(invalid_cat23)} parody rows missing Author field.\")\n",
    "\n",
    "# 4️⃣ Trending date ISO check\n",
    "invalid_dates = raw_batch_df[\n",
    "    ~raw_batch_df[\"trending_date\"].astype(str).str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "]\n",
    "if not invalid_dates.empty:\n",
    "    issues.append(f\"{len(invalid_dates)} rows have invalid trending_date format.\")\n",
    "\n",
    "# 5️⃣ Publish date/time columns exist\n",
    "missing_cols = [c for c in [\"publish_date\", \"publish_time\"] if c not in raw_batch_df.columns]\n",
    "if missing_cols:\n",
    "    issues.append(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "# 6️⃣ Dropped columns check\n",
    "expected_drops = {\n",
    "    \"tags\", \"thumbnail_link\", \"ratings_disabled\",\n",
    "    \"video_error_or_removed\", \"comments_disabled\", \"description\"\n",
    "}\n",
    "still_there = pre_cols.intersection(raw_batch_df.columns).intersection(expected_drops)\n",
    "if still_there:\n",
    "    issues.append(f\"Columns not dropped: {still_there}\")\n",
    "\n",
    "# --- Report Results ---\n",
    "if issues:\n",
    "    print(f\"⚠️ Issues found in batch {date_str}:\")\n",
    "    for i in issues:\n",
    "        print(f\"   - {i}\")\n",
    "else:\n",
    "    print(f\"✅ Batch {date_str} passed all acceptance checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b08c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = transform_root / f\"day={date_str}_validation.txt\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    if issues:\n",
    "        f.write(f\"⚠️ Issues in batch {date_str}:\\n\")\n",
    "        for i in issues:\n",
    "            f.write(f\" - {i}\\n\")\n",
    "    else:\n",
    "        f.write(f\"✅ Batch {date_str} passed all checks.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a548f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction total: 0, Transformation total: 39058\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "❌ Global row mismatch across batches!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m total_transformed = \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pd.read_csv(f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m transform_root.glob(\u001b[33m\"\u001b[39m\u001b[33mday=*.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtraction total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_extracted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Transformation total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_transformed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m total_extracted == total_transformed, \u001b[33m\"\u001b[39m\u001b[33m❌ Global row mismatch across batches!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: ❌ Global row mismatch across batches!"
     ]
    }
   ],
   "source": [
    "total_extracted = sum(len(pd.read_csv(f)) for f in extract_root.glob(\"day=*/videos.csv\"))\n",
    "total_transformed = sum(len(pd.read_csv(f)) for f in transform_root.glob(\"day=*.csv\"))\n",
    "print(f\"Extraction total: {total_extracted}, Transformation total: {total_transformed}\")\n",
    "assert total_extracted == total_transformed, \"❌ Global row mismatch across batches!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c9a51",
   "metadata": {},
   "source": [
    "##LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc2b21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_batch_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cat = \u001b[43mraw_batch_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcategory_id\u001b[39m\u001b[33m'\u001b[39m].unique()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnique categories in batch:\u001b[39m\u001b[33m\"\u001b[39m, cat)\n",
      "\u001b[31mNameError\u001b[39m: name 'raw_batch_df' is not defined"
     ]
    }
   ],
   "source": [
    "cat = raw_batch_df['category_id'].unique()\n",
    "print(\"Unique categories in batch:\", cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
